{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0672d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedropertusi/Desktop/nlp-final-project/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821f6c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon:  #train=454763  #test=113691\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 1: pick‐and‐load your dataset ────────────────────────────────────────\n",
    "\n",
    "# 1) change this to 'ag', 'amazon' or 'imdb'\n",
    "# DATASET = 'ag'  \n",
    "DATASET = 'amazon'\n",
    "# DATASET = 'imdb'\n",
    "\n",
    "TEST_SIZE = 0.2  # fraction to hold out as “test” when you don't have a built‐in test split\n",
    "\n",
    "configs = {\n",
    "    'ag': {\n",
    "        'path': 'dataset/ag-news-classification-dataset',\n",
    "        'train_file': 'train.csv',\n",
    "        'test_file':  'test.csv',\n",
    "        'text_cols':  ['Title','Description'],\n",
    "        'label_col':  'Class Index',\n",
    "        'label_shift': -1,\n",
    "        'has_test_file': True\n",
    "    },\n",
    "    'amazon': {\n",
    "        'path': 'dataset/amazon-fine-food-reviews',\n",
    "        'train_file': 'Reviews.csv',\n",
    "        'test_file':  None,\n",
    "        'text_cols':  ['Summary','Text'],\n",
    "        'label_col':  'Score',\n",
    "        'label_transform': lambda x: int(x)-1,\n",
    "        'has_test_file': False\n",
    "    },\n",
    "    'imdb': {\n",
    "        'path': 'dataset/imdb-dataset-of-50k-movie-reviews',\n",
    "        'train_file': 'IMDB Dataset.csv',\n",
    "        'test_file':  None,\n",
    "        'text_cols':  ['review'],\n",
    "        'label_col':  'sentiment',\n",
    "        'label_transform': lambda x: 1 if x=='positive' else 0,\n",
    "        'has_test_file': False\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = configs[DATASET]\n",
    "\n",
    "# load train\n",
    "train_df = pd.read_csv(f\"{cfg['path']}/{cfg['train_file']}\")\n",
    "# load test if provided\n",
    "if cfg['has_test_file']:\n",
    "    test_df  = pd.read_csv(f\"{cfg['path']}/{cfg['test_file']}\")\n",
    "\n",
    "# build train_texts\n",
    "if len(cfg['text_cols'])>1:\n",
    "    train_texts = (train_df[cfg['text_cols'][0]] + \" \" + train_df[cfg['text_cols'][1]]).tolist()\n",
    "else:\n",
    "    train_texts = train_df[cfg['text_cols'][0]].tolist()\n",
    "\n",
    "# build train_labels\n",
    "if 'label_shift' in cfg:\n",
    "    train_labels = (train_df[cfg['label_col']] + cfg['label_shift']).tolist()\n",
    "else:\n",
    "    train_labels = train_df[cfg['label_col']].map(cfg['label_transform']).tolist()\n",
    "\n",
    "# handle test_texts / test_labels\n",
    "if cfg['has_test_file']:\n",
    "    if len(cfg['text_cols'])>1:\n",
    "        test_texts = (test_df[cfg['text_cols'][0]] + \" \" + test_df[cfg['text_cols'][1]]).tolist()\n",
    "    else:\n",
    "        test_texts = test_df[cfg['text_cols'][0]].tolist()\n",
    "    if 'label_shift' in cfg:\n",
    "        test_labels = (test_df[cfg['label_col']] + cfg['label_shift']).tolist()\n",
    "    else:\n",
    "        test_labels = test_df[cfg['label_col']].map(cfg['label_transform']).tolist()\n",
    "else:\n",
    "    split_idx   = int(len(train_texts)*(1-TEST_SIZE))\n",
    "    test_texts  = train_texts[split_idx:]\n",
    "    test_labels = train_labels[split_idx:]\n",
    "    train_texts = train_texts[:split_idx]\n",
    "    train_labels= train_labels[:split_idx]\n",
    "\n",
    "print(f\"{DATASET}:  #train={len(train_texts)}  #test={len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f57901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess the text\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to extract embeddings\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e805bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 2: Generate & save TRAINING embeddings ─────────────────────────────\n",
    "fractions = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# assumes: DATASET, fractions, train_texts, model, tokenizer are already defined\n",
    "\n",
    "for frac in fractions:\n",
    "    n = int(len(train_texts) * frac)\n",
    "    embs = []\n",
    "    print(f\"→ {DATASET}: generating {int(frac*100)}% training embeddings ({n} samples)\")\n",
    "    for txt in tqdm(train_texts[:n], leave=False):\n",
    "        e = get_embeddings(txt, model, tokenizer)\n",
    "        embs.append(e.detach().numpy())\n",
    "    embs = np.array(embs)\n",
    "    np.save(f\"{DATASET}_bert_embeddings_{int(frac*100)}.npy\", embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece8af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ amazon: generating test embeddings (113691 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 29608/113691 [19:05<42:43, 32.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 31881/113691 [20:33<40:35, 33.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 48500/113691 [31:18<31:17, 34.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 53517/113691 [46:42<1048:04:54, 62.70s/it]"
     ]
    }
   ],
   "source": [
    "# ─── Cell 3: Generate & save TEST embeddings ─────────────────────────────────\n",
    "\n",
    "# assumes: DATASET, test_texts, model, tokenizer are already defined\n",
    "\n",
    "print(f\"→ {DATASET}: generating test embeddings ({len(test_texts)} samples)\")\n",
    "test_embs = []\n",
    "for txt in tqdm(test_texts, leave=False):\n",
    "    try:\n",
    "        e = get_embeddings(txt, model, tokenizer)\n",
    "        test_embs.append(e.detach().numpy())\n",
    "    except Exception as j:\n",
    "        print(f\"Error processing text\")\n",
    "\n",
    "test_embs = np.array(test_embs)\n",
    "np.save(f\"{DATASET}_bert_embeddings_test.npy\", test_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad11459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 3: train LogisticRegression and eval on the fixed test set ─────────\n",
    "\n",
    "test_embs  = np.load(f\"{DATASET}_bert_embeddings_test.npy\")\n",
    "test_lbls  = np.array(test_labels)\n",
    "\n",
    "for frac in fractions:\n",
    "    train_embs = np.load(f\"{DATASET}_bert_embeddings_{int(frac*100)}.npy\")\n",
    "    train_lbls = np.array(train_labels[:len(train_embs)])\n",
    "    clf = LogisticRegression(max_iter=1_000)\n",
    "    clf.fit(train_embs, train_lbls)\n",
    "    y_pred = clf.predict(test_embs)\n",
    "    print(f\"\\n=== {DATASET} | {int(frac*100)}% train ===\")\n",
    "    print(classification_report(test_lbls, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
