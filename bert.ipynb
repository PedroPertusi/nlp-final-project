{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Load the data\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the train.csv and test.csv are in 'dataset/ag-news-classification-dataset'\n",
    "train_df = pd.read_csv('dataset/ag-news-classification-dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/ag-news-classification-dataset/test.csv')\n",
    "\n",
    "# Combine Title and Description into one text field\n",
    "train_texts = (train_df['Title'] + \" \" + train_df['Description']).tolist()\n",
    "test_texts = (test_df['Title'] + \" \" + test_df['Description']).tolist()\n",
    "\n",
    "# Labels are Class Index but shifted down by 1 to get 0-based classes\n",
    "train_labels = (train_df['Class Index'] - 1).tolist()\n",
    "test_labels = (test_df['Class Index'] - 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f57901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Preprocess the text\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to extract embeddings\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "# Generate embeddings for fractions of data\n",
    "fractions = [1.0]\n",
    "for frac in fractions:\n",
    "    print(f\"Generating embeddings for {int(frac*100)}% of the training data.\")\n",
    "    \n",
    "    # Subsample the training data\n",
    "    subset_size = int(len(train_texts) * frac)\n",
    "    subset_texts = train_texts[:subset_size]\n",
    "    \n",
    "    embeddings = []\n",
    "    for i in tqdm(range(len(subset_texts))):\n",
    "        e = get_embeddings(subset_texts[i], model, tokenizer)\n",
    "        embeddings.append(e.detach().numpy())\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    np.save(f'bert_embeddings_ag_{int(frac*100)}.npy', embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load embeddings and train the classifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train and test model for different fractions\n",
    "for frac in fractions:\n",
    "    print(f\"\\nTraining with {int(frac*100)}% data\")\n",
    "    \n",
    "    # Load the embeddings for the current fraction\n",
    "    embeddings = np.load(f'bert_embeddings_{int(frac*100)}.npy')\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings, train_labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a classifier\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Print the classification report\n",
    "    print(f\"Classification Report for {int(frac*100)}% training data:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
