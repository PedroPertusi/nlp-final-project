{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3d6e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Change this to 'amazon', 'ag', or 'imdb'\n",
    "# DATASET = 'amazon'\n",
    "# DATASET = 'ag'\n",
    "DATASET = 'amazon'\n",
    "\n",
    "TEST_SIZE = 0.2  # fraction for test set\n",
    "\n",
    "configs = {\n",
    "    'ag': {\n",
    "        'path': 'dataset/ag-news-classification-dataset',\n",
    "        'train_file': 'train.csv',\n",
    "        'test_file':  'test.csv',\n",
    "        'text_cols':  ['Title','Description'],\n",
    "        'label_col':  'Class Index',\n",
    "        'label_shift': -1,\n",
    "        'has_test_file': True\n",
    "    },\n",
    "    'amazon': {\n",
    "        'path': 'dataset/amazon-fine-food-reviews',\n",
    "        'train_file': 'Reviews.csv',\n",
    "        'test_file':  None,\n",
    "        'text_cols':  ['Text'],\n",
    "        'label_col':  'Score',\n",
    "        'has_test_file': False\n",
    "    },\n",
    "    'imdb': {\n",
    "        'path': 'dataset/imdb-dataset-of-50k-movie-reviews',\n",
    "        'train_file': 'IMDB Dataset.csv',\n",
    "        'test_file':  None,\n",
    "        'text_cols':  ['review'],\n",
    "        'label_col':  'sentiment',\n",
    "        'label_transform': lambda x: 1 if x=='positive' else 0,\n",
    "        'has_test_file': False\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = configs[DATASET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "727d3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon: #train=454763  #test=113691\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and split dataset\n",
    "# ────────────────────────────────\n",
    "\n",
    "# load train\n",
    "train_df = pd.read_csv(f\"{cfg['path']}/{cfg['train_file']}\")\n",
    "\n",
    "# build train_texts\n",
    "if len(cfg['text_cols']) > 1:\n",
    "    texts = train_df[cfg['text_cols'][0]].astype(str) + \" \" + train_df[cfg['text_cols'][1]].astype(str)\n",
    "else:\n",
    "    texts = train_df[cfg['text_cols'][0]].astype(str)\n",
    "\n",
    "# build train_labels\n",
    "if 'label_shift' in cfg:\n",
    "    labels = (train_df[cfg['label_col']] + cfg['label_shift']).tolist()\n",
    "elif 'label_transform' in cfg:\n",
    "    labels = train_df[cfg['label_col']].map(cfg['label_transform']).tolist()\n",
    "else:\n",
    "    labels = train_df[cfg['label_col']].tolist()\n",
    "\n",
    "# split into train/test\n",
    "if cfg['has_test_file']:\n",
    "    # built‐in test split\n",
    "    test_df = pd.read_csv(f\"{cfg['path']}/{cfg['test_file']}\")\n",
    "    if len(cfg['text_cols']) > 1:\n",
    "        test_texts = test_df[cfg['text_cols'][0]].astype(str) + \" \" + test_df[cfg['text_cols'][1]].astype(str)\n",
    "    else:\n",
    "        test_texts = test_df[cfg['text_cols'][0]].astype(str)\n",
    "    if 'label_shift' in cfg:\n",
    "        test_labels = (test_df[cfg['label_col']] + cfg['label_shift']).tolist()\n",
    "    elif 'label_transform' in cfg:\n",
    "        test_labels = test_df[cfg['label_col']].map(cfg['label_transform']).tolist()\n",
    "    else:\n",
    "        test_labels = test_df[cfg['label_col']].tolist()\n",
    "\n",
    "    train_texts = texts.tolist()\n",
    "    train_labels = labels\n",
    "else:\n",
    "    # sequential split: first (1–TEST_SIZE) for train, last TEST_SIZE for test\n",
    "    split_idx = int(len(texts) * (1 - TEST_SIZE))\n",
    "    train_texts = texts.tolist()[:split_idx]\n",
    "    train_labels = labels[:split_idx]\n",
    "    test_texts  = texts.tolist()[split_idx:]\n",
    "    test_labels = labels[split_idx:]\n",
    "\n",
    "print(f\"{DATASET}: #train={len(train_texts)}  #test={len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70f93985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load GloVe Embeddings\n",
    "# ────────────────────────────────\n",
    "\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_path = 'glove.6B/glove.6B.100d.txt'\n",
    "embeddings_index = load_glove_embeddings(glove_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d68707b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare Data with Tokenizer\n",
    "# ────────────────────────────────\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Tokenize train and test texts\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "X_test = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "y_train = np.array(y_train) - 1\n",
    "y_test = np.array(y_test) - 1\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "96b6b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Training on 20% of the data...\n",
      "Epoch 1/5\n",
      "\u001b[1m11369/11369\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 11ms/step - accuracy: 0.6503 - loss: 0.9893\n",
      "Epoch 2/5\n",
      "\u001b[1m11369/11369\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 11ms/step - accuracy: 0.6967 - loss: 0.8042\n",
      "Epoch 3/5\n",
      "\u001b[1m11369/11369\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.7176 - loss: 0.7410\n",
      "Epoch 4/5\n",
      "\u001b[1m11369/11369\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 12ms/step - accuracy: 0.7333 - loss: 0.6977\n",
      "Epoch 5/5\n",
      "\u001b[1m11369/11369\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 12ms/step - accuracy: 0.7472 - loss: 0.6626\n",
      "\u001b[1m3553/3553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step\n",
      "→ Training on 40% of the data...\n",
      "Epoch 1/5\n",
      "\u001b[1m22739/22739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 13ms/step - accuracy: 0.6680 - loss: 0.9205\n",
      "Epoch 2/5\n",
      "\u001b[1m22739/22739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 13ms/step - accuracy: 0.7188 - loss: 0.7455\n",
      "Epoch 3/5\n",
      "\u001b[1m22739/22739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 14ms/step - accuracy: 0.7363 - loss: 0.6935\n",
      "Epoch 4/5\n",
      "\u001b[1m22739/22739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 13ms/step - accuracy: 0.7499 - loss: 0.6584\n",
      "Epoch 5/5\n",
      "\u001b[1m22739/22739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 13ms/step - accuracy: 0.7570 - loss: 0.6380\n",
      "\u001b[1m3553/3553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step\n",
      "→ Training on 60% of the data...\n",
      "Epoch 1/5\n",
      "\u001b[1m34108/34108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 13ms/step - accuracy: 0.6802 - loss: 0.8869\n",
      "Epoch 2/5\n",
      "\u001b[1m34108/34108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 13ms/step - accuracy: 0.7296 - loss: 0.7156\n",
      "Epoch 3/5\n",
      "\u001b[1m34108/34108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 12ms/step - accuracy: 0.7433 - loss: 0.6760\n",
      "Epoch 4/5\n",
      "\u001b[1m34108/34108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 12ms/step - accuracy: 0.7539 - loss: 0.6472\n",
      "Epoch 5/5\n",
      "\u001b[1m34108/34108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 12ms/step - accuracy: 0.7626 - loss: 0.6243\n",
      "\u001b[1m3553/3553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step\n",
      "→ Training on 80% of the data...\n",
      "Epoch 1/5\n",
      "\u001b[1m45477/45477\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 12ms/step - accuracy: 0.6878 - loss: 0.8604\n",
      "Epoch 2/5\n",
      "\u001b[1m45477/45477\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 12ms/step - accuracy: 0.7367 - loss: 0.6958\n",
      "Epoch 3/5\n",
      "\u001b[1m45477/45477\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 12ms/step - accuracy: 0.7500 - loss: 0.6553\n",
      "Epoch 4/5\n",
      "\u001b[1m45477/45477\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 12ms/step - accuracy: 0.7617 - loss: 0.6259\n",
      "Epoch 5/5\n",
      "\u001b[1m45477/45477\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 12ms/step - accuracy: 0.7696 - loss: 0.6072\n",
      "\u001b[1m3553/3553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step\n",
      "            accuracy  error_rate  rel_err_reduction\n",
      "fraction_%                                         \n",
      "20          0.731940    0.268060           0.000000\n",
      "40          0.742873    0.257127           4.078619\n",
      "60          0.747755    0.252245           5.899724\n",
      "80          0.759154    0.240846          10.152251\n",
      "→ Saved ULMFiT‐style metrics to ./glove/amazon/results/glove_ulmfit_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Summarize GloVe results with ULMFiT‐style metrics\n",
    "# ───────────────────────────────────────────────────────\n",
    "\n",
    "fractions = [0.2, 0.4, 0.6, 0.8]\n",
    "rows = []\n",
    "baseline_frac = fractions[0]\n",
    "baseline_error = None\n",
    "num_classes = 5\n",
    "\n",
    "for frac in fractions:\n",
    "    print(f\"→ Training on {int(frac * 100)}% of the data...\")\n",
    "    n = int(len(train_labels) * frac)\n",
    "    X_frac = X_train[:n]\n",
    "    y_frac = y_train[:n]\n",
    "\n",
    "    # Build a fresh model for each run\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=len(word_index) + 1,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  input_length=maxlen,\n",
    "                  trainable=False),\n",
    "        LSTM(64),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_frac, y_frac, epochs=5, batch_size=8)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred_class)\n",
    "    err = 1.0 - acc\n",
    "\n",
    "    if frac == baseline_frac:\n",
    "        baseline_error = err\n",
    "    rel = (baseline_error - err) / baseline_error * 100 if baseline_error else 0.0\n",
    "\n",
    "    rows.append({\n",
    "        \"fraction_%\":        int(frac*100),\n",
    "        \"accuracy\":          acc,\n",
    "        \"error_rate\":        err,\n",
    "        \"rel_err_reduction\": rel\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).set_index(\"fraction_%\")\n",
    "print(df)\n",
    "\n",
    "# save to CSV\n",
    "results_dir = f\"./glove/{DATASET}/results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "output_path = os.path.join(results_dir, \"glove_ulmfit_metrics.csv\")\n",
    "df.to_csv(output_path)\n",
    "print(f\"→ Saved ULMFiT‐style metrics to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
